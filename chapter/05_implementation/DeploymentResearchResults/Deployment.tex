\section{Deployment of the front end}

\subsection{Introduction and reasoning}
In the context of Development Operations engineering, application deployment is a process of making developed software accessible beyond the team responsible for its development. In practice, this often means uploading the code to the server, which runs your program, and exposing it through an endpoint, which can be used as a part of RESTful API call (if it's a backend/database) or as an URL (if it is frontend). While containerisation and deployment of our backend service was a necessary step to ease the development process for the frontend, the deployment of our frontend itself was necessary for different reasons. Firstly, front-end deployment provides an opportunity for testers/focus groups to have an opportunity to view the application remotely and from different devices. Secondly, as our team was interested in scaling the applicaiton, finding a way to deploy it was a necessary first step.

\subsection{Available options that didn't work}
The official Next.js documentation page \cite{vercelMainDeploymentMainPage} provides a number of options to deploy the front end. The first idea was to deploy to GitHub pages as a static export. Using GitHub Actions it would be was potentially possible to create a continuous deployment pathway upon merge to main. However, in our case, choosing this option would require a code refactor to comply with the list of permitted features. For the same general reason, Cloudflare Pages and Cloudflare Workers \cite{cloudflare_pages_workers} were discarded as an option, although they promised high compatibility with the Next.js framework.
 It specifically required an adaptor or code-wide changes to adapt the application to be run on their servers, which would create a problem if we or the client decide to change a provider. A major barrier for searching for a deployment provider was a paywall: a predominant majority of providers didn't have free plans, or the free plans available weren't suited for our needs. These issues prompted us to explore deployment via containerisation. 

\subsection{Deployment via containerisation}
Containerisation is a type of software virtualisation. In contrast to a full Virtual Machine (VM), where a full guest operating system (OS) is run on virtualised hardware, containers package the application and its user-space dependencies while sharing the hostâ€™s kernel \cite{containerisationArticle1, containerisationArticle2}. As there is no need to handle an entire operating system, containerised apps require fewer CPU and RAM resources of a host system, while allowing the application to be run on any software regardless of host setup. This solves the issue of the code not being adapted to be build and run by a deployment service providers.

Next.js documentation also provides general steps that are can be taken in successful containerization and deployment of the project \cite{vercelMainDeploymentMainPage, nextjs_docker_example}. The only two necessary additions to the code, neither of which changed how the application worked, were the Dockerfile (taken from the GitHub referenced in documentation \cite{nextjs_dockerfile}) and \texttt{output: "standalone"} line to the next.config.js file. To create an image of the project, the command \texttt{"docker build -t <image name>"} was used when navigating to the root of the project. To create and run a container from the image, \texttt{"docker create --name <container name> -p 3000:3000 <image name>"} and \texttt{"docker start <container name>"} was used. With this settings, the application is accessible at \texttt{"http://localhost:3000"}. To expose the localhost port to the Internet, we used Ngrok, which is a software that handles port exposure securely \cite{ngrokMain} \cite{ngrokMainDocs}. Command \texttt{"ngrok http http://localhost:3000"} created a link, following which the app UI becomes accessible through the browser. For the purposes of this project, the self-hosting option is viable, as long as there is a possibility to set aside a device that would run the container without interruptions.

The second option for container deployment is through a container hosting provider, and there is a wide range of paid and free-tier plans \cite{dockerDeploymentServices}. Because Docker adheres to the Open Container Initiative (OCI) specifications \cite{OCIdocker}, which is a modern standard for building, running and distributing containers, the choice to switch providers would have minimal consequences. We chose Google Cloud as a provider, because of its generous free plan. To deploy the image there, first, Artifact Repository was created and image was uploaded using the gcloud CLI. For the image to be uploadable, it firstly has to be tagged in this format: \texttt{"\lstinline|docker tag IMAGE_NAME:TAG LOCATION_NAME-docker.pkg.dev/PROJECT_NAME/REPOSITORY_NAME/IMAGE_NAME:TAG|"} (according to documentation \cite{GoogleCloudRunDocs}). Names of location, project and repository can be copied from the artifact repository as a path. To push the tagged image, use \lstinline|"docker push IMAGE_NAME:TAG LOCATION_NAME-docker.pkg.dev/PROJECT_NAME/REPOSITORY_NAME/IMAGE_NAME:TAG"| command. After this is done, it is possible to create a new service in Google Cloud Run and select the image form the Artifact registry. Once the new service is created, the application can be accessed through the link using the browser. 

This result showed our team that deployment through containerisation is possible, and thus it was a successful proof-of-concept. However, the standard of modern DevOps is the creation of a Continuous Integration (CI)/Continuous Deployment (CD) pipeline. Our team has considered creating the CD using containerisation method; however, due to the time constraints and low priority of the job, it has not been implemented.